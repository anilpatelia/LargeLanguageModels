{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b795a7-2d46-46bb-9fd1-9a41a8255922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install git+https://github.com/huggingface/datasets.git\\n!pip install git+https://github.com/huggingface/transformers.git\\n!pip install torchaudio\\n!pip install librosa\\n!pip install jiwer\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install git+https://github.com/huggingface/datasets.git\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install torchaudio\n",
    "!pip install librosa\n",
    "!pip install jiwer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "666049f9-1a2a-4234-ae53-58b29cb23fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ta\", trust_remote_code=True, split=\"train+validation\")\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ta\", trust_remote_code=True, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1d8eaed-d6bd-480c-a272-2d145fe666a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1041826-d5b6-48fe-b044-81a94d822275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b347fe48-b858-4f18-95af-fe02f4259627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b8704f-2f07-4abb-b7b7-0f1b0d98a167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'path': '/home/vmadmin/.cache/huggingface/datasets/downloads/extracted/6e584255450c6ef089111d1246a070c397022fd026f7bbcba10d9ca510b3f859/ta_train_0/common_voice_ta_26769796.mp3', 'array': [-9.094947017729282e-13, -6.366462912410498e-12, -1.1368683772161603e-11, -6.59383658785373e-12, 5.684341886080801e-13, 4.320099833421409e-12, 7.44648787076585e-12, 1.0231815394945443e-11, 9.43600753089413e-12, 1.2732925824820995e-11, 2.2509993868879974e-11, 2.887645678129047e-11, 2.637534635141492e-11, 2.546585164964199e-11, 2.546585164964199e-11, 2.3419488570652902e-11, 1.6541434888495132e-11, 6.480149750132114e-12, -1.887201506178826e-11, -4.4565240386873484e-11, -5.729816621169448e-11, -5.502442945726216e-11, -4.774847184307873e-11, -2.97859514830634e-11, -6.821210263296962e-13, 3.1604940886609256e-11, 6.616573955398053e-11, 9.617906471248716e-11, 1.0254552762489766e-10, 7.969447324285284e-11, 4.376943252282217e-11, -7.844391802791506e-12, -7.048583938740194e-11, -1.1300471669528633e-10, -1.340367816737853e-10, -1.552962203277275e-10, -1.7257661966141313e-10, -1.7598722479306161e-10, -1.8917489796876907e-10, -2.0554580260068178e-10, -1.950866135302931e-10, -1.6189005691558123e-10, -1.2232703738845885e-10, -5.88897819397971e-11, 3.126388037344441e-11, 1.297451035497943e-10, 2.2291146706265863e-10, 2.9831426218152046e-10, 3.4640379453776404e-10, 3.9113956518121995e-10, 4.4428816181607544e-10, 4.240519047016278e-10, 2.2589574655285105e-10, -1.0697931429604068e-10, -4.0313352656085044e-10, -5.118181434227154e-10, -4.3314685171935707e-10, -2.856950231944211e-10, -2.0907009457005188e-10, -2.6761881599668413e-10, -4.5292836148291826e-10, -7.039488991722465e-10, -9.613358997739851e-10, -1.203261490445584e-09, -1.4424585970118642e-09, -1.6680132830515504e-09, -1.7698766896501184e-09, -1.5606929082423449e-09, -9.467839845456183e-10, -1.20508047984913e-10, 5.536548997042701e-10, 9.113136911764741e-10, 1.0660414773155935e-09, 1.0907115211011842e-09, 8.146798791131005e-10, 4.9112713895738125e-11, -1.067974153556861e-09, -2.0513652998488396e-09, -2.5065673980861902e-09, -2.49178810918238e-09, -2.323986336705275e-09, -2.149590727640316e-09, -1.86355464393273e-09, -1.3956196198705584e-09, -8.803908713161945e-10, -5.456968210637569e-10, -5.566107574850321e-10, -9.831637726165354e-10, -1.680291461525485e-09, -2.1668711269740015e-09, -1.8062564777210355e-09, -3.878994903061539e-10, 1.4904344425303861e-09, 2.8869635571027175e-09, 3.365244083397556e-09, 3.2534899219172075e-09, 3.0595401767641306e-09, 2.8478552849264815e-09, 2.329329618078191e-09, 1.401076588081196e-09, ...], 'sampling_rate': 48000}</td>\n",
       "      <td>‡ÆÜ‡ÆØ‡Æø‡Æ©‡ØÅ‡ÆÆ‡Øç ‡ÆÖ‡Æï‡Øç‡Æï‡Ææ‡Æ≤‡Æ§‡Øç‡Æ§‡ØÅ ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ≤‡Øç‡Æ≤‡Øã‡Æ∞‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Øä‡Æ§‡ØÅ‡Æµ‡Ææ‡Æï ‡ÆÆ‡ØÇ‡Æü‡Æ∞‡Øç‡Æï‡Æ≥‡Øç.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'path': '/home/vmadmin/.cache/huggingface/datasets/downloads/extracted/6e584255450c6ef089111d1246a070c397022fd026f7bbcba10d9ca510b3f859/ta_train_0/common_voice_ta_26036480.mp3', 'array': [2.842170943040401e-14, 1.2505552149377763e-12, 1.8758328224066645e-12, 1.4210854715202004e-12, 9.094947017729282e-13, 7.105427357601002e-13, 7.887024366937112e-13, 9.805489753489383e-13, 1.2221335055073723e-12, 1.4637180356658064e-12, 1.7692514120426495e-12, 1.5205614545266144e-12, 6.679101716144942e-13, 2.5579538487363607e-13, 8.526512829121202e-13, 8.242295734817162e-13, 3.694822225952521e-13, -1.7053025658242404e-13, -1.4779288903810084e-12, -2.9558577807620168e-12, -3.069544618483633e-12, -1.9468870959826745e-12, -1.0516032489249483e-12, -2.7711166694643907e-13, 1.7053025658242404e-13, 5.968558980384842e-13, 1.8474111129762605e-12, 3.858247055177344e-12, 5.357492227631155e-12, 5.5138116294983774e-12, 4.490630090003833e-12, 1.5063505998114124e-12, -1.9184653865522705e-12, -3.225864020350855e-12, -2.6432189770275727e-12, -2.2453150450019166e-12, -3.268496584496461e-12, -7.474909580196254e-12, -1.4424017535930034e-11, -1.9731771772057982e-11, -2.0548895918182097e-11, -1.757882728270488e-11, -1.3358203432289883e-11, -1.254818471352337e-11, -1.9028334463655483e-11, -3.015543370565865e-11, -3.936406756110955e-11, -4.298783551348606e-11, -3.929656600121234e-11, -2.9615421226480976e-11, -2.1792345705762273e-11, -2.0616397478079307e-11, -2.2676971411783597e-11, -2.4712676349736284e-11, -2.6858515411731787e-11, -2.7966962079517543e-11, -2.745537130977027e-11, -2.445688096486265e-11, -1.3431034062705294e-11, 9.49995637711254e-12, 3.650768576335395e-11, 4.9823256631498225e-11, 3.183231456205249e-11, -2.1110224679432577e-11, -8.787992555880919e-11, -1.3221779227023944e-10, -1.3136514098732732e-10, -8.839151632855646e-11, -2.984279490192421e-11, 6.565414878423326e-12, 6.536993168992922e-13, -3.163336259603966e-11, -5.17843545821961e-11, -3.538502824085299e-11, 7.30437932361383e-12, 3.666400516522117e-11, 1.4495071809506044e-11, -5.994138518872205e-11, -1.4347278920467943e-10, -1.7905676941154525e-10, -1.3505996321327984e-10, -2.717115421546623e-11, 9.134737410931848e-11, 1.624584911041893e-10, 1.5870682545937598e-10, 9.36211108637508e-11, 3.637978807091713e-12, -7.688072400924284e-11, -1.3386625141720288e-10, -1.673470251262188e-10, -1.7882939573610201e-10, -1.701323526503984e-10, -1.516866632300662e-10, -1.4409806681214832e-10, -1.680007244431181e-10, -2.2669865984425996e-10, -2.9528735012718244e-10, -3.296349859738257e-10, -2.96381585940253e-10, -2.0872903405688703e-10, ...], 'sampling_rate': 48000}</td>\n",
       "      <td>‡Æµ‡ØÜ‡ÆØ‡Øç‡Æ∑‡Øã ‡Æ§‡Ææ‡Æµ‡Øã ‡Æö‡ØÄ‡Æ©‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡Æï‡ØÅ‡Æ±‡Øà‡Æ®‡Øç‡Æ§ ‡Æµ‡ÆØ‡Æ§‡ØÅ ‡Æï‡Øä‡Æ£‡Øç‡Æü ‡Æé‡Æ∞‡Æø‡ÆÆ‡Æ≤‡Øà ‡Æ§‡ØÄ‡Æµ‡ØÅ ‡ÆÜ‡Æï‡ØÅ‡ÆÆ‡Øç.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show_random_elements(common_voice_train.remove_columns([\"path\"]), num_examples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "376c8f21-d191-4008-bf22-5ff00197d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\‚Äú\\%\\‚Äò\\‚Äù\\ÔøΩ]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dc8b92f-bf14-4d60-a624-a48da9f635fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(remove_special_characters, remove_columns=[\"sentence\"])\n",
    "common_voice_test = common_voice_test.map(remove_special_characters, remove_columns=[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4253e66c-6655-4ac2-8352-6366077d4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"text\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36adf25b-9da7-4785-8ab9-434587dbbc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536d483f931448fcb5c6eea2b3c8f313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcace9b431545d3b088624be006a3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
    "vocab_test = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4b24af2-20f0-412b-969b-22e4b53cb254",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bde26cc-62f6-405f-88b5-701d5c2aeb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca245785-b33b-4c45-aec2-d5aa060ce44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'¬æ': 0,\n",
       " '‡Æì': 1,\n",
       " '‡Æè': 2,\n",
       " '‚óè': 3,\n",
       " 'j': 4,\n",
       " '‡Øà': 5,\n",
       " 'b': 6,\n",
       " '‚óØ': 7,\n",
       " '≈°': 8,\n",
       " 'z': 9,\n",
       " '‡Øã': 10,\n",
       " '‡ÆÉ': 11,\n",
       " 'h': 12,\n",
       " 'i': 13,\n",
       " 'x': 14,\n",
       " ')': 15,\n",
       " '‡Æ≥': 16,\n",
       " '‡Æ∞': 17,\n",
       " '‚Äî': 18,\n",
       " 'v': 19,\n",
       " '‡Æê': 20,\n",
       " '‡Æø': 21,\n",
       " '‡Æö': 22,\n",
       " '‡Æ∏': 23,\n",
       " '‡ØÄ': 24,\n",
       " 'm': 25,\n",
       " '_': 26,\n",
       " 'l': 27,\n",
       " '‡Æ£': 28,\n",
       " '√°': 29,\n",
       " '√¥': 30,\n",
       " '‡Æµ': 31,\n",
       " '‡Æû': 32,\n",
       " '‡ÆÜ': 33,\n",
       " '‡ØÅ': 34,\n",
       " '‡ØÇ': 35,\n",
       " '‚Ä¶': 36,\n",
       " '‡Æü': 37,\n",
       " '‡Æä': 38,\n",
       " '‡Æ™': 39,\n",
       " 'p': 40,\n",
       " '‡Æ©': 41,\n",
       " '‡Øá': 42,\n",
       " '‡Æé': 43,\n",
       " 'n': 44,\n",
       " 't': 45,\n",
       " '‡Æî': 46,\n",
       " '‡¥•': 47,\n",
       " 'q': 48,\n",
       " '‡Æ®': 49,\n",
       " '‡Æ∑': 50,\n",
       " '‡Ææ': 51,\n",
       " '‡ÆÖ': 52,\n",
       " 'u': 53,\n",
       " '‚Äö': 54,\n",
       " '‚Ä¢': 55,\n",
       " '‚Äô': 56,\n",
       " '√©': 57,\n",
       " 'c': 58,\n",
       " '‡Æô': 59,\n",
       " 'f': 60,\n",
       " '‡Øå': 61,\n",
       " '‡Æ±': 62,\n",
       " '‡Æ≤': 63,\n",
       " 'r': 64,\n",
       " 'o': 65,\n",
       " '\\\\': 66,\n",
       " 's': 67,\n",
       " 'k': 68,\n",
       " '‡Æâ': 69,\n",
       " '‡Æí': 70,\n",
       " 'e': 71,\n",
       " '&': 72,\n",
       " '(': 73,\n",
       " '‡Øä': 74,\n",
       " '‡ÆÆ': 75,\n",
       " 'd': 76,\n",
       " '‡Øç': 77,\n",
       " 'w': 78,\n",
       " 'g': 79,\n",
       " '‡Æ¥': 80,\n",
       " '‡Æ§': 81,\n",
       " '`': 82,\n",
       " '‡Æá': 83,\n",
       " '‡Æà': 84,\n",
       " 'a': 85,\n",
       " '‡ØÜ': 86,\n",
       " 'y': 87,\n",
       " '‡Øó': 88,\n",
       " ' ': 89,\n",
       " '‡Æπ': 90,\n",
       " '‚Äì': 91,\n",
       " '‡Æï': 92,\n",
       " '‡Æú': 93,\n",
       " '¬∑': 94,\n",
       " '‡ÆØ': 95,\n",
       " \"'\": 96}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5a30a2-dac3-4bfd-ade1-4b2a9699eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4879935-d348-4f6f-b941-075b0756c8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0c97adb-9fa0-4d3f-8b82-5d10ee0600da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f00ac326-d0d6-4706-9db0-51d31fe60aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84d5cbe2-a65a-4883-b284-1e0749354b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e6c1240-50ec-45be-8629-001203a32dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e79d56d-3109-4c89-ad33-1d922da76abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " processor.save_pretrained(\"wav2vec2-large-xlsr-tamil-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e06e44b-cd0a-420b-ac58-dcadb392ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"speech\"] = speech_array[0].numpy()\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"text\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c2f5668-a117-475f-bdd3-dec4584f8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(speech_file_to_array_fn, remove_columns=common_voice_train.column_names)\n",
    "common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5291cf-c502-4998-b2e5-3b33a4514bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03ff187d-72fa-4da6-bea3-af5c153dd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def resample(batch):\n",
    "    speech_array = np.asarray(batch[\"speech\"])\n",
    "    batch[\"speech\"] = librosa.resample(speech_array, orig_sr=48_000, target_sr=16_000)\n",
    "    batch[\"sampling_rate\"] = 16_000\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5b51c99-88b7-448b-893a-b5c248e170b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(resample, num_proc=4)\n",
    "common_voice_test = common_voice_test.map(resample, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8870c-a9f2-4059-aefc-323096939188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017261e-e732-4437-ba95-60c187dd0045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84c2c96f-8e7c-47f4-8a11-dd65f232f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(batch[\"sampling_rate\"])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec5ab0f6-4cac-4fad-908b-1ee1e1f1d60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6af584da70b418388028c2c79454771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/53468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fffa30a210442c87ca2d920355c942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/11815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names, batch_size=8, num_proc=4, batched=True)\n",
    "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8b60c39-cdaa-48b7-aaea-e4f67a83a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf053718-f62f-4d53-b785-a5654e03b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94f11b08-19d2-4020-aeea-4064b3f05683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom datasets import load_metric\\nwer_metric = load_metric(\"wer\")\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\"\"\"\n",
    "from datasets import load_metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b888ca11-9c57-4c41-90dd-af3293b642ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./venv/lib/python3.11/site-packages (from evaluate) (2.20.1.dev0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from evaluate) (2.0.1)\n",
      "Requirement already satisfied: dill in ./venv/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./venv/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./venv/lib/python3.11/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.11/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in ./venv/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./venv/lib/python3.11/site-packages (from evaluate) (0.24.5)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m996.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "998050d2-85de-4fb8-a036-e77f3b0816e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8524cd26-8dca-4588-b4fb-be16d04ce6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7aa9837-c9ba-485e-851f-53988e6fb240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2177: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bec9619a-77cb-427b-9959-d6473babab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  #output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-tamil-demo\",\n",
    "  output_dir=\"./wav2vec2-large-xlsr-Tamil-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=2,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  fp16=True,\n",
    "  save_steps=400,\n",
    "  eval_steps=400,\n",
    "  logging_steps=400,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0df613db-a7c0-4bcc-97fb-9c09e9e8c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c9aa7f1-bb57-472a-aa16-7fc73e5c5239",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3066402069.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[37], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    function ConnectButton(){\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47aa69e-bd55-45dd-9ea9-658e81ffb698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmadmin/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vmadmin/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='121' max='50130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  121/50130 2:46:44 < 1167:55:35, 0.01 it/s, Epoch 0.07/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
