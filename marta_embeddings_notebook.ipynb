{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b113b75",
      "metadata": {
        "id": "7b113b75"
      },
      "source": [
        "###         ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd6c6e8",
      "metadata": {
        "id": "7bd6c6e8"
      },
      "outputs": [],
      "source": [
        "# Run this cell, then hide it before the presentation\n",
        "my_api_key = ''\n",
        "secret_word = 'embeddings'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51cc2b9-feea-49fe-bc89-6feeeafefe21",
      "metadata": {
        "id": "a51cc2b9-feea-49fe-bc89-6feeeafefe21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4434cbeb-7da2-4395-9b04-e9694c7c082e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/221.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/221.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.7\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.347-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.1,>=0.0.11 (from langchain)\n",
            "  Downloading langchain_core-0.0.11-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.4/181.4 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.69-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.347 langchain-core-0.0.11 langsmith-0.0.69 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.18-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.3)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.1-cp37-abi3-manylinux_2_28_x86_64.whl (699 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m699.4/699.4 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from chromadb)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n",
            "Collecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=327e5c6b503563ebd19a1a1d205f89bf600bdf4df972485953def928bdc8123f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, urllib3, typing-extensions, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, humanfriendly, httptools, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, asgiref, posthog, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, kubernetes, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.1 chroma-hnswlib-0.7.3 chromadb-0.4.18 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.104.1 httptools-0.6.1 humanfriendly-10.0 kubernetes-28.1.0 mmh3-4.0.1 monotonic-1.6 onnxruntime-1.16.3 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.1.0 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 typing-extensions-4.8.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ],
      "source": [
        "# # Uncomment this cell to get everything installed in colab.\n",
        "# # You will get a bunch of logs and errors.  Don't worry about them.  Everything will be installed properly in the end.\n",
        "\n",
        "%pip install openai\n",
        "%pip install langchain\n",
        "%pip install numpy\n",
        "%pip install chromadb\n",
        "%pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb1a3592",
      "metadata": {
        "id": "eb1a3592"
      },
      "source": [
        "### French Toast"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae83628",
      "metadata": {
        "id": "1ae83628"
      },
      "source": [
        "When I was in college, my friends and I had a favorite game to play during long car trips. It was called French Toast.\n",
        "\n",
        "<img src=https://www.donwoodlock.com/ml301-Nov2023/marta_embeddings_presentation/image1.png width=\"700\">    \n",
        "\n",
        "<br/><br/>\n",
        "\n",
        "<img src=https://www.donwoodlock.com/ml301-Nov2023/marta_embeddings_presentation/image2.png width=\"700\">\n",
        "\n",
        "<br/><br/>\n",
        "\n",
        "<img src=https://www.donwoodlock.com/ml301-Nov2023/marta_embeddings_presentation/image3.png width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee839a3",
      "metadata": {
        "id": "cee839a3"
      },
      "source": [
        "Natural language models like ChatGPT are great at playing French Toast!\n",
        "* the models map every single word to a point in a multi-dimensional space;\n",
        "* the closer two words are in meaning, the closer their corresponding points are;\n",
        "* this mapping is called an embedding.\n",
        "\n",
        "<img src=https://www.donwoodlock.com/ml301-Nov2023/marta_embeddings_presentation/image4.png width=\"700\">\n",
        "\n",
        "Source: https://openai.com/blog/introducing-text-and-code-embeddings\n",
        "</br></br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "552d7666",
      "metadata": {
        "id": "552d7666"
      },
      "source": [
        "### Embeddings ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb694244",
      "metadata": {
        "id": "cb694244"
      },
      "source": [
        "Words in the plot above are represented in a 3-dimensional space.</br>\n",
        "In practice, the embeddings used in modern Natural Language Models (NLMs) have thousands of dimensions. The original vectors for the embedding above had 2048 dimensions!</br>\n",
        "</br>\n",
        "  \n",
        "We can try out embeddings ourselves. ChatGPT provides an API to convert text into its corresponding embedding.</br>\n",
        "Let's start by importing some code dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb6c6b6",
      "metadata": {
        "id": "4eb6c6b6"
      },
      "outputs": [],
      "source": [
        "# you might need to run: pip install langchain\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Use your own API key here\n",
        "embedding_module = OpenAIEmbeddings(openai_api_key = my_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c1389e6",
      "metadata": {
        "id": "7c1389e6"
      },
      "source": [
        "We imported the OpenAIEmbeddings tool. It uses a newer embedding model, which has 1536 dimensions.</br>\n",
        "The function below takes a string of text and returns its embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b18e73b3",
      "metadata": {
        "id": "b18e73b3"
      },
      "outputs": [],
      "source": [
        "# you might need to run: pip install numpy\n",
        "import numpy as np\n",
        "\n",
        "def text_embedding(text: str) -> np.ndarray:\n",
        "    return np.array(embedding_module.embed_documents([text])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69a8d081",
      "metadata": {
        "id": "69a8d081"
      },
      "source": [
        "Get the embedding of \"french toast\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8e9633",
      "metadata": {
        "id": "4c8e9633",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0656ed4e-ec5d-4748-8ae4-751e9a77ecd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.00523991,  0.00127193,  0.01377016, ...,  0.0225271 ,\n",
              "        0.01101095, -0.02228097])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "my_embedding = text_embedding(\"french toast\")\n",
        "my_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1ae6188",
      "metadata": {
        "id": "b1ae6188"
      },
      "source": [
        "Check the dimensionality of the embedding vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba3383c1",
      "metadata": {
        "id": "ba3383c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f820c308-6e38-4afd-ed69-33cb05a6c754"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(my_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871e772e",
      "metadata": {
        "id": "871e772e"
      },
      "source": [
        "</br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fec2ccd",
      "metadata": {
        "id": "2fec2ccd"
      },
      "source": [
        "### Euclidean Distance ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aec8957b",
      "metadata": {
        "id": "aec8957b"
      },
      "source": [
        "Words with similar meanings are close to each other in the embedding space.\n",
        "\n",
        "<img src=https://www.donwoodlock.com/ml301-Nov2023/marta_embeddings_presentation/image5.png width=\"500\">\n",
        "\n",
        "Let's test it! Similar words should have a smaller euclidean distance.\n",
        "\n",
        "<img src=https://www.donwoodlock.com/ml301-Nov2023/marta_embeddings_presentation/image6.png width=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd20608d",
      "metadata": {
        "id": "cd20608d"
      },
      "outputs": [],
      "source": [
        "def distance(vector_a, vector_b):\n",
        "    return np.linalg.norm(vector_a - vector_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45efe1b2",
      "metadata": {
        "id": "45efe1b2"
      },
      "source": [
        "Let's say the secret word is \"cake\".\n",
        "Is it closer to \"french toast\", or is it closer to \"InterSystems IRIS\"? </br>\n",
        "All distances are normalized between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb6a3a8",
      "metadata": {
        "id": "aeb6a3a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12743ff-08d9-4b87-fe7b-581d33e28f16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5828727431872559"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "embedding_1 = text_embedding(\"french toast\")\n",
        "embedding_2 = text_embedding(\"cake\")\n",
        "\n",
        "distance(embedding_1, embedding_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "764ec8e9",
      "metadata": {
        "id": "764ec8e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a54e9f-5575-42f2-a92f-26a20b00e745"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7115007800881934"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "embedding_1 = text_embedding(\"InterSystems IRIS\")\n",
        "embedding_2 = text_embedding(\"cake\")\n",
        "\n",
        "distance(embedding_1, embedding_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46931560",
      "metadata": {
        "id": "46931560"
      },
      "source": [
        "Another popular metric is the cosine similarity between two vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39dfd717",
      "metadata": {
        "id": "39dfd717"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vector_a, vector_b):\n",
        "    return np.dot(vector_a, vector_b) / (np.linalg.norm(vector_a) * np.linalg.norm(vector_b))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "498b333b",
      "metadata": {
        "id": "498b333b"
      },
      "source": [
        "</br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "602070e0",
      "metadata": {
        "id": "602070e0"
      },
      "source": [
        "### How are embeddings generated? ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c245dcc",
      "metadata": {
        "id": "1c245dcc"
      },
      "source": [
        "**Step 1: clean up your data**\n",
        "\n",
        "Start with a corpus of text. </br>\n",
        "\n",
        "Remove:\n",
        "* punctuation\n",
        "* stopwords (\"is\", \"are\", \"a\", \"the\", etc...)\n",
        "* numbers\n",
        "\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bfdefd2",
      "metadata": {
        "id": "0bfdefd2"
      },
      "source": [
        "**Step 2: use a training algorithm**\n",
        "\n",
        "Training algorithms generate an intial mapping.</br>\n",
        "A famous family of embedding algorithms is called Word2Vec. The algorithms looks at a sliding interval of words in a corpus of text.\n",
        "\n",
        "<img src=https://www.donwoodlock.com/ml301-Nov2023/marta_embeddings_presentation/theory3.png width=\"700\">\n",
        "\n",
        "How does a given word influence the probability of other words appearing in the same interval? </br>\n",
        "\n",
        "The stronger the correlation between the appearence of the two words, then closer the model will place them in the embedding space. </br>\n",
        "The algorithm used a neural network to re-organize the words in space. </br>\n",
        "\n",
        "Word2Vec has now been replaced by more complex \"transformer\" models. \"GPT\" stands for \"Generative Pre-trained Transformers\".\n",
        "\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f28d8ab1",
      "metadata": {
        "id": "f28d8ab1"
      },
      "source": [
        "**Step 3: pick a function for sentence embedding**\n",
        "\n",
        "We have talked about embeddings for individual words, but we can calculate an embedding for a sentence or a paragraph, too. </br>\n",
        "The embedding of a sentence is still a single vector, a single point in the embedding space. </br>\n",
        "\n",
        "Embeddings are used to measure similarity between text elements. </br>\n",
        "The closer the two elements are in meaning, the closer they will be in the embedding space. We can compare a word to a whole paragraph.\n",
        "\n",
        "<img src=https://www.donwoodlock.com/ml301-Nov2023/marta_embeddings_presentation/theory1.png width=\"700\">\n",
        "\n",
        "A sentence's embedding is usually some weighted average of the words it contains. </br>\n",
        "Words that carry the most meaning should have a higher weight. </br></br>\n",
        "\n",
        "Some formulas are optimized for a given style of sentence. The OpenAI API has a dedicated method to embed queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b74151d",
      "metadata": {
        "id": "0b74151d"
      },
      "outputs": [],
      "source": [
        "def query_embedding(text: str) -> np.ndarray:\n",
        "    return np.array(embedding_module.embed_query(text))\n",
        "\n",
        "question = query_embedding(\"How can I cancel my shoes order?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83333449",
      "metadata": {
        "id": "83333449"
      },
      "source": [
        "</br>\n",
        "\n",
        "### Use case examples ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "343183dc",
      "metadata": {
        "id": "343183dc"
      },
      "source": [
        "We just learned how to embed a question. Now let's see how we can use embeddings to find an answer. </br>\n",
        "Look at the three paragraphs below. Which one is most relevant to the question?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c24e17",
      "metadata": {
        "id": "57c24e17"
      },
      "outputs": [],
      "source": [
        "paragraphs = [\"\"\"It's easier than you think to make restaurant-quality French toast in the comfort of your own kitchen.\n",
        "You just need a skillet and a few staple ingredients. The best breads for French toast are brioche, sourdough, French bread, or challah.\n",
        "French toast is traditionally made with day-old slices because they absorb the eggy mixture better than fresh ones.\"\"\",\n",
        "\n",
        "\"\"\"If your order has not shipped yet, you can easily cancel it from our website. Log into your account, navigate to the\n",
        "Orders section, select your order, and click Cancel. If your order has already shipped, you will be able to return the item after you\n",
        "receive it. You can print a pre-paid mailing label on our website, and drop off the package at any post office.\"\"\",\n",
        "\n",
        "\"\"\"InterSystems IRIS makes it easier to build high-performance, machine learning-enabled applications that connect data\n",
        "and application silos.It provides high performance database management, interoperability, and analytics capabilities, all built-in\n",
        "from the ground up to speed and simplify your most demanding data-intensive applications.\"\"\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7dd09b",
      "metadata": {
        "id": "dc7dd09b"
      },
      "source": [
        "We can:\n",
        "1. use the API to embed each of the paragraphs above;\n",
        "2. calculate the euclidean distance between each paragraph and the question \"\"How can cancel shoes my shoes order?\";\n",
        "3. display the paragraph which is closest in meaning to the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e46f593f",
      "metadata": {
        "id": "e46f593f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8538d59-86e8-412e-e6bf-b36389bfcebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If your order has not shipped yet, you can easily cancel it from our website. Log into your account, navigate to the\n",
            "Orders section, select your order, and click Cancel. If your order has already shipped, you will be able to return the item after you\n",
            "receive it. You can print a pre-paid mailing label on our website, and drop off the package at any post office.\n"
          ]
        }
      ],
      "source": [
        "distances = []\n",
        "\n",
        "for par in paragraphs:\n",
        "    embedding = text_embedding(par)\n",
        "    distances.append(distance(embedding, question))\n",
        "\n",
        "print(paragraphs[np.argmin(distances)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92969f16",
      "metadata": {
        "id": "92969f16"
      },
      "source": [
        "</br>\n",
        "\n",
        "Confluence provides another example of how embeddings can be useful in our day-to-day life.</br>\n",
        "When we type a new page title, the Confluence UI will display a list of pages with similar titles.\n",
        "\n",
        "<img src=https://www.donwoodlock.com/ml301-Nov2023/marta_embeddings_presentation/theory2.png width=\"700\">\n",
        "\n",
        "Pre-computing an embedding for each page's title can be an efficient way to later tell if two pages are similar. </br>\n",
        "Disclaimer: I do not know what technology Confluence is using for this feature.\n",
        "</br></br></br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544d4477",
      "metadata": {
        "id": "544d4477"
      },
      "source": [
        "### Let's play! ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9f4c2f",
      "metadata": {
        "id": "2a9f4c2f"
      },
      "source": [
        "The function below allows us to play the French Toast game with ChatGPT embeddings. </br>\n",
        "I have already set a secret_word variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "294b4b7e",
      "metadata": {
        "id": "294b4b7e"
      },
      "outputs": [],
      "source": [
        "def is_it_more_like(word_1, word_2):\n",
        "    if secret_word == word_1 or secret_word == word_2:\n",
        "        return \"It's \"+secret_word+\"!\"\n",
        "    distance_1 = distance(text_embedding(secret_word), text_embedding(word_1))\n",
        "    distance_2 = distance(text_embedding(secret_word), text_embedding(word_2))\n",
        "    if distance_1 < distance_2:\n",
        "        return \"It's more like \"+word_1+\".\"\n",
        "    else:\n",
        "        return \"It's more like \"+word_2+\".\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be2bc2af",
      "metadata": {
        "id": "be2bc2af"
      },
      "source": [
        "I am thinking of something, and it's not French toast!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee8e201",
      "metadata": {
        "id": "cee8e201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d55d9ee0-bcb2-4095-e643-84e40037cbe6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's more like videogames.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "is_it_more_like('french toast', 'videogames')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7d4c574",
      "metadata": {
        "id": "a7d4c574"
      },
      "source": [
        "</br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "711c0d66",
      "metadata": {
        "id": "711c0d66"
      },
      "source": [
        "### Thank you!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}