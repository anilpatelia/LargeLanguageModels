{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\n# load model and processor\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\nmodel.config.forced_decoder_ids = None\n\n# load dummy dataset and read audio files\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\nsample = ds[0][\"audio\"]\ninput_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n\n# generate token ids\npredicted_ids = model.generate(input_features)\n# decode token ids to text\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T13:29:03.298560Z","iopub.execute_input":"2024-08-23T13:29:03.299007Z","iopub.status.idle":"2024-08-23T13:29:38.334104Z","shell.execute_reply.started":"2024-08-23T13:29:03.298969Z","shell.execute_reply":"2024-08-23T13:29:38.333150Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom transformers import WhisperProcessor, pipeline\nfrom datasets import load_dataset\nimport evaluate\nimport tempfile\nimport os\n\n# Step 1: Load the common_voice dataset for Tamil\ndataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ta\", split=\"test\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:17:33.466934Z","iopub.execute_input":"2024-08-23T14:17:33.467720Z","iopub.status.idle":"2024-08-23T14:17:34.818266Z","shell.execute_reply.started":"2024-08-23T14:17:33.467681Z","shell.execute_reply":"2024-08-23T14:17:34.817315Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"dataset[0][\"audio\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-23T13:49:44.058117Z","iopub.execute_input":"2024-08-23T13:49:44.058796Z","iopub.status.idle":"2024-08-23T13:49:44.072427Z","shell.execute_reply.started":"2024-08-23T13:49:44.058755Z","shell.execute_reply":"2024-08-23T13:49:44.071633Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"{'path': '/root/.cache/huggingface/datasets/downloads/extracted/256dec25b50e8072243364b2a76958f3978dd0a3272161faa2272dcf50c8fa20/ta_test_0/common_voice_ta_26622621.mp3',\n 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n        3.34414472e-05, 2.36578981e-05, 1.05088511e-05]),\n 'sampling_rate': 48000}"},"metadata":{}}]},{"cell_type":"code","source":"import torchaudio\n\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech_array[0].numpy()\n    batch[\"sampling_rate\"] = sampling_rate\n    batch[\"target_text\"] = batch[\"sentence\"]\n    return batch\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:17:41.023856Z","iopub.execute_input":"2024-08-23T14:17:41.024263Z","iopub.status.idle":"2024-08-23T14:17:41.029781Z","shell.execute_reply.started":"2024-08-23T14:17:41.024225Z","shell.execute_reply":"2024-08-23T14:17:41.028623Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(speech_file_to_array_fn, remove_columns=dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:17:43.996227Z","iopub.execute_input":"2024-08-23T14:17:43.996955Z","iopub.status.idle":"2024-08-23T14:17:44.007057Z","shell.execute_reply.started":"2024-08-23T14:17:43.996914Z","shell.execute_reply":"2024-08-23T14:17:44.006037Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport numpy as np\n\ndef resample(batch):\n    speech_array = np.asarray(batch[\"speech\"])\n    batch[\"speech\"] = librosa.resample(speech_array, orig_sr=48_000, target_sr=16_000)\n    batch[\"sampling_rate\"] = 16_000\n    return batch","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:17:50.318155Z","iopub.execute_input":"2024-08-23T14:17:50.318834Z","iopub.status.idle":"2024-08-23T14:17:50.325131Z","shell.execute_reply.started":"2024-08-23T14:17:50.318791Z","shell.execute_reply":"2024-08-23T14:17:50.323780Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(resample, num_proc=4)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:18:00.651436Z","iopub.execute_input":"2024-08-23T14:18:00.651872Z","iopub.status.idle":"2024-08-23T14:29:49.574344Z","shell.execute_reply.started":"2024-08-23T14:18:00.651830Z","shell.execute_reply":"2024-08-23T14:29:49.572907Z"},"trusted":true},"execution_count":94,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/11815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"061098ce69da4f7a89c2bcf5009d30ec"}},"metadata":{}}]},{"cell_type":"code","source":"sample = dataset[2]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:37:38.242815Z","iopub.execute_input":"2024-08-23T14:37:38.243243Z","iopub.status.idle":"2024-08-23T14:37:38.295921Z","shell.execute_reply.started":"2024-08-23T14:37:38.243204Z","shell.execute_reply":"2024-08-23T14:37:38.294938Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ta\", task=\"transcribe\")\n\n\n# load dummy dataset and read audio files\n#ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n#sample = ds[0][\"audio\"]\n#sample = dataset[0]\n#input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \ninput_features = processor(sample[\"speech\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n\n# generate token ids\npredicted_ids = model.generate(input_features)\n# decode token ids to text\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\nprint(transcription)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\nprint(transcription)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:39:15.703845Z","iopub.execute_input":"2024-08-23T14:39:15.704251Z","iopub.status.idle":"2024-08-23T14:39:54.192127Z","shell.execute_reply.started":"2024-08-23T14:39:15.704209Z","shell.execute_reply":"2024-08-23T14:39:54.191079Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"['<|startoftranscript|><|ta|><|transcribe|><|notimestamps|> அந்த பாட்டின் இரண்டாவது பகத்தில் இதை வண்ணது திரிவலை ஆடலை சொல்லுகிறார்']\n[' அந்த பாட்டின் இரண்டாவது பகத்தில் இதை வண்ணது திரிவலை ஆடலை சொல்லுகிறார்']\n","output_type":"stream"}]},{"cell_type":"code","source":"def transcribe(batch):\n    input_features = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"], return_tensors=\"pt\").input_features \n\n    # generate token ids\n    with torch.no_grad():\n        predicted_ids = model.generate(input_features)\n    # decode token ids to text\n    #transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n    #print(transcription)\n    batch[\"transcription\"] = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n    #print(transcription)\n    return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}