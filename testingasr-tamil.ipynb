{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://huggingface.co/Harveenchadha/vakyansh-wav2vec2-tamil-tam-250","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:39:54.763339Z","iopub.execute_input":"2024-08-23T09:39:54.763905Z","iopub.status.idle":"2024-08-23T09:40:10.535149Z","shell.execute_reply.started":"2024-08-23T09:39:54.763857Z","shell.execute_reply":"2024-08-23T09:40:10.532797Z"}}},{"cell_type":"code","source":"#!pip install jiwer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should return True if CUDA is available\nprint(torch.version.cuda)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:48:19.453699Z","iopub.execute_input":"2024-08-23T09:48:19.454467Z","iopub.status.idle":"2024-08-23T09:48:22.957060Z","shell.execute_reply.started":"2024-08-23T09:48:19.454421Z","shell.execute_reply":"2024-08-23T09:48:22.956153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\ntest_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ta\", split=\"test\", trust_remote_code=True)\nwer = load_metric(\"wer\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:56:48.525111Z","iopub.execute_input":"2024-08-23T09:56:48.525547Z","iopub.status.idle":"2024-08-23T09:56:49.374312Z","shell.execute_reply.started":"2024-08-23T09:56:48.525489Z","shell.execute_reply":"2024-08-23T09:56:49.373404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"Harveenchadha/vakyansh-wav2vec2-tamil-tam-250\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"Harveenchadha/vakyansh-wav2vec2-tamil-tam-250\")\nmodel.to(\"cuda\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:56:56.671640Z","iopub.execute_input":"2024-08-23T09:56:56.672661Z","iopub.status.idle":"2024-08-23T09:57:04.339595Z","shell.execute_reply.started":"2024-08-23T09:56:56.672619Z","shell.execute_reply":"2024-08-23T09:57:04.338686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resampler = torchaudio.transforms.Resample(48_000, 16_000)\n\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“]'\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n  batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n  speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n  batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n  return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:57:45.112819Z","iopub.execute_input":"2024-08-23T09:57:45.113705Z","iopub.status.idle":"2024-08-23T09:59:28.399438Z","shell.execute_reply.started":"2024-08-23T09:57:45.113665Z","shell.execute_reply":"2024-08-23T09:59:28.398475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n  inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n  with torch.no_grad():\n      logits = model(inputs.input_values.to(\"cuda\")).logits\n\n      pred_ids = torch.argmax(logits, dim=-1)\n      batch[\"pred_strings\"] = processor.batch_decode(pred_ids, skip_special_tokens=True)\n      return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:59:34.321126Z","iopub.execute_input":"2024-08-23T09:59:34.321508Z","iopub.status.idle":"2024-08-23T10:14:36.527565Z","shell.execute_reply.started":"2024-08-23T09:59:34.321472Z","shell.execute_reply":"2024-08-23T10:14:36.526566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://huggingface.co/Rajaram1996/wav2vec2-large-xlsr-53-tamil","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T10:16:09.477247Z","iopub.execute_input":"2024-08-23T10:16:09.477687Z","iopub.status.idle":"2024-08-23T10:16:09.483343Z","shell.execute_reply.started":"2024-08-23T10:16:09.477648Z","shell.execute_reply":"2024-08-23T10:16:09.482184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ta\", split=\"test\", trust_remote_code=True)\nwer = load_metric(\"wer\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T10:16:15.922952Z","iopub.execute_input":"2024-08-23T10:16:15.923341Z","iopub.status.idle":"2024-08-23T10:16:17.036924Z","shell.execute_reply.started":"2024-08-23T10:16:15.923305Z","shell.execute_reply":"2024-08-23T10:16:17.035937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"Rajaram1996/wav2vec2-large-xlsr-53-tamil\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"Rajaram1996/wav2vec2-large-xlsr-53-tamil\")\n\nmodel.to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-08-23T10:16:36.841281Z","iopub.execute_input":"2024-08-23T10:16:36.841719Z","iopub.status.idle":"2024-08-23T10:17:07.544618Z","shell.execute_reply.started":"2024-08-23T10:16:36.841681Z","shell.execute_reply":"2024-08-23T10:17:07.543613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chars_to_ignore_regex = '[\\\\\\\\,\\\\\\\\?\\\\\\\\.\\\\\\\\!\\\\\\\\-\\\\\\\\;\\\\\\\\:\\\\\\\\\"\\\\\\\\“]'\n\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T10:17:13.532148Z","iopub.execute_input":"2024-08-23T10:17:13.532546Z","iopub.status.idle":"2024-08-23T10:19:04.259612Z","shell.execute_reply.started":"2024-08-23T10:17:13.532494Z","shell.execute_reply":"2024-08-23T10:19:04.258582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))","metadata":{"execution":{"iopub.status.busy":"2024-08-23T10:19:07.536550Z","iopub.execute_input":"2024-08-23T10:19:07.536943Z","iopub.status.idle":"2024-08-23T10:37:59.369237Z","shell.execute_reply.started":"2024-08-23T10:19:07.536909Z","shell.execute_reply":"2024-08-23T10:37:59.368197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****https://huggingface.co/Amrrs/wav2vec2-large-xlsr-53-tamil","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\n#test_dataset = load_dataset(\"common_voice\", \"ta\", split=\"test[:2%]\")\ntest_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ta\", split=\"test\", trust_remote_code=True)\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"Amrrs/wav2vec2-large-xlsr-53-tamil\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"Amrrs/wav2vec2-large-xlsr-53-tamil\")\nmodel.to(\"cuda\")\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\"\"\"\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\n\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))","metadata":{"execution":{"iopub.status.busy":"2024-08-23T10:19:07.536550Z","iopub.execute_input":"2024-08-23T10:19:07.536943Z","iopub.status.idle":"2024-08-23T10:37:59.369237Z","shell.execute_reply.started":"2024-08-23T10:19:07.536909Z","shell.execute_reply":"2024-08-23T10:37:59.368197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://huggingface.co/vasista22/whisper-tamil-medium","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\n\n# path to the audio file to be transcribed\naudio = \"/path/to/audio.format\"\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ntranscribe = pipeline(task=\"automatic-speech-recognition\", model=\"vasista22/whisper-tamil-medium\", chunk_length_s=30, device=device)\ntranscribe.model.config.forced_decoder_ids = transcribe.tokenizer.get_decoder_prompt_ids(language=\"ta\", task=\"transcribe\")\n\nprint('Transcription: ', transcribe(audio)[\"text\"])\n","metadata":{},"execution_count":null,"outputs":[]}]}